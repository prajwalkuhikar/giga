PS C:\Users\XRSpecial\gigapose-1> python test.py test_dataset_name=tless run_id=$NAME_RUN
[2024-08-21 11:30:48,078][__main__][INFO] - Initializing logger, callbacks and trainer
[2024-08-21 11:30:48,080][__main__][INFO] - Tensorboard logger initialized at ./gigaPose_datasets/results/large_/gigapose
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[2024-08-21 11:30:48,116][__main__][INFO] - Trainer initialized!
Using cache found in C:\Users\XRSpecial/.cache\torch\hub\facebookresearch_dinov2_main
[2024-08-21 11:30:49,160][xformers][WARNING] - A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
[2024-08-21 11:30:49,169][dinov2][INFO] - using MLP layer as FFN
[2024-08-21 11:30:51,239][src.models.network.ae_net][INFO] - Initialize AENet done!
[2024-08-21 11:30:51,326][src.models.network.ist_net][INFO] - Init for Regressor with done!
[2024-08-21 11:30:51,370][src.models.network.ist_net][INFO] - Init weights for ISTNet done!
[2024-08-21 11:30:51,371][src.models.network.ist_net][INFO] - Init for ISTNet done!
[2024-08-21 11:30:51,374][src.models.gigaPose][INFO] - Initialize GigaPose done!
[2024-08-21 11:30:51,374][__main__][INFO] - Model initialized!
[2024-08-21 11:30:51,423][src.dataloader.test][INFO] - Split: test_primesense for tless!
[2024-08-21 11:30:51,424][src.custom_megapose.web_scene_dataset][INFO] - WebSceneDataset: 4 shards
[2024-08-21 11:30:51,424][src.custom_megapose.web_scene_dataset][INFO] - IterableWebSceneDataset: 1000 samples
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 20135.88it/s] 
[2024-08-21 11:30:51,427][src.custom_megapose.template_dataset][INFO] - Loaded 30 template datas
[2024-08-21 11:30:52,773][src.utils.inout][INFO] - Loading test list from gigaPose_datasets\datasets\tless\test_targets_bop19.json
4904it [00:02, 1867.53it/s]
[2024-08-21 11:30:55,403][src.utils.inout][INFO] - Detections: 4904 test samples!
[2024-08-21 11:30:55,462][src.dataloader.keypoints][INFO] - Initialized normalized center patch done!
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 30196.57it/s] 
[2024-08-21 11:30:55,473][src.custom_megapose.template_dataset][INFO] - Loaded 30 template datas
[2024-08-21 11:30:55,474][__main__][INFO] - Dataloaders initialized!
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at ./gigaPose_datasets/pretrained/gigaPose_v1.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at ./gigaPose_datasets/pretrained/gigaPose_v1.ckpt
Error executing job with overrides: ['test_dataset_name=tless', 'run_id=']
Traceback (most recent call last):
  File "C:\Users\XRSpecial\gigapose-1\test.py", line 87, in <module>
    run_test()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\main.py", line 94, in decorated_main
    _run_hydra(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\utils.py", line 394, in _run_hydra
    _run_app(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\utils.py", line 457, in _run_app
    run_and_report(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\utils.py", line 223, in run_and_report
    raise ex
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\utils.py", line 220, in run_and_report
    return func()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\_internal\hydra.py", line 132, in run
    _ = ret.return_value
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\core\utils.py", line 260, in return_value
    raise self._return_value
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\hydra\core\utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "C:\Users\XRSpecial\gigapose-1\test.py", line 77, in run_test
    trainer.test(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 753, in test
    return call._call_and_handle_interrupt(
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\trainer\call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 793, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 986, in _run
    results = self._run_stage()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1023, in _run_stage
    return self._evaluation_loop.run()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\loops\utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\loops\evaluation_loop.py", line 113, in run
    self.reset()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\loops\evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\loops\fetchers.py", line 104, in __iter__
    super().__iter__()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\loops\fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\utilities\combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\utilities\combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\pytorch_lightning\utilities\combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\torch\utils\data\dataloader.py", line 442, in __iter__
    return self._get_iterator()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\torch\utils\data\dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\site-packages\torch\utils\data\dataloader.py", line 1043, in __init__
    w.start()
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\context.py", line 327, in _Popen
    return Popen(process_obj)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\popen_spawn_win32.py", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'webdataset.pipeline.DataPipeline_Length'>: attribute lookup DataPipeline_Length on webdataset.pipeline failed
PS C:\Users\XRSpecial\gigapose-1> Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "C:\Users\XRSpecial\miniconda3\envs\gigapose\lib\multiprocessing\spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
